from model.gan_model import *
import tensorflow as tf
import argparse
from model.datapipeline import InputHandler
AUTOTUNE = tf.data.experimental.AUTOTUNE

def parse_args():
    desc = "Tensorflow implementation of BigGAN"
    parser = argparse.ArgumentParser(description=desc)
    parser.add_argument('--phase', type=str, default='train', help='train or test ?')
    parser.add_argument('--dataset', type=str, default='celebA-HQ', help='[mnist / cifar10 / custom_dataset]')

    parser.add_argument('--epoch', type=int, default=1, help='The number of epochs to run')
    parser.add_argument('--batch_size', type=int, default=1, help='The size of batch per gpu')
    parser.add_argument('--ch', type=int, default=64, help='base channel number per layer')

    parser.add_argument('--print_freq', type=int, default=1000, help='The number of image_print_freqy')
    parser.add_argument('--save_freq', type=int, default=1000, help='The number of ckpt_save_freq')

    parser.add_argument('--g_lr', type=float, default=0.00005, help='learning rate for generator')
    parser.add_argument('--d_lr', type=float, default=0.0002, help='learning rate for discriminator')

    parser.add_argument('--beta1', type=float, default=0.0, help='beta1 for Adam optimizer')
    parser.add_argument('--beta2', type=float, default=0.9, help='beta2 for Adam optimizer')
    parser.add_argument('--moving_decay', type=float, default=0.9999, help='moving average decay for generator')

    parser.add_argument('--z_dim', type=int, default=128, help='Dimension of noise vector')
    # parser.add_argument('--sn', type=str2bool, default=True, help='using spectral norm')

    # parser.add_argument('--gan_type', type=str, default='hinge', help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge]')
    # parser.add_argument('--ld', type=float, default=10.0, help='The gradient penalty lambda')

    parser.add_argument('--n_critic', type=int, default=1, help='The number of critic')

    parser.add_argument('--img_size', type=int, default=256, help='The size of image')
    parser.add_argument('--sample_num', type=int, default=64, help='The number of sample images')

    parser.add_argument('--test_num', type=int, default=10, help='The number of images generated by the test')

    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',
                        help='Directory name to save the checkpoints')
    parser.add_argument('--result_dir', type=str, default='results',
                        help='Directory name to save the generated images')
    parser.add_argument('--log_dir', type=str, default='logs',
                        help='Directory name to save training logs')
    parser.add_argument('--sample_dir', type=str, default='samples',
                        help='Directory name to save the samples on training')

    return parser.parse_args()


def main():
    # parse arguments
    args = parse_args()
    if args is None:
      exit()

    # Take input
    input_pipeline = InputHandler(root_path='../../Data/dog_cat_small')
    ds = input_pipeline.build_ds()
    ds = ds.batch(1)
    image_batch, label_batch = next(iter(ds))

    # default gan = BigGAN_128
    gan = BigGAN256(args)

    z = tf.initializers.TruncatedNormal()(shape=[args.batch_size, 1, 1, args.z_dim])

    # build graph
    model = DiscriminatorBlock(ch=64, use_bias=True, sn=True)
    model.trainable = False
    # model = GeneratorBlock(z=z, z_dim=args.z_dim, ch=args.ch, c_dim=3)
    # model.train_on_batch()
    model.compile(optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy'],
    )
    model.fit(
        image_batch,
        label_batch,
        epochs=1
    )
    model.summary()

    if args.phase == 'train':
        # launch the graph in a session
        # gan.train()

        # visualize learned generator
        # gan.visualize_results(args.epoch - 1)

        print(" [*] Training finished!")

    if args.phase == 'test' :
        gan.test()
        print(" [*] Test finished!")


if __name__ == '__main__':
    main()
